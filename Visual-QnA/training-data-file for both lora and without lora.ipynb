{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers nltk numpy datasets==2.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from datasets import load_dataset, set_caching_enabled\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    AutoModel, AutoConfig,  \n",
    "    TrainingArguments, Trainer,\n",
    "    logging\n",
    ")\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "import subprocess\n",
    "import safetensors\n",
    "try:\n",
    "    nltk.data.find('wordnet.zip')\n",
    "except:\n",
    "    nltk.download('wordnet', download_dir='/kaggle/working/')\n",
    "    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n",
    "    subprocess.run(command.split())\n",
    "    nltk.data.path.append('/kaggle/working/')\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
    "\n",
    "set_caching_enabled(True)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"csv\", \n",
    "    data_files={\n",
    "        \"train\": \"/kaggle/input/preprocessed/processed/data_train.csv\",\n",
    "        \"test\": \"/kaggle/input/preprocessed/processed/data_eval.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "with open(\"/kaggle/input/preprocessed/processed/answer_space.txt\") as f:\n",
    "    answer_space = f.read().splitlines()\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda examples: {\n",
    "        'label': [\n",
    "            answer_space.index(ans.replace(\" \", \"\").split(\",\")[0])  # select the 1st answer if multiple answers are provided\n",
    "            for ans in examples['answer']\n",
    "        ]\n",
    "    },\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as display\n",
    "\n",
    "def show_example(train=True, idx=None):\n",
    "    dataset_type = \"train\" if train else \"test\"\n",
    "    data = dataset[dataset_type]\n",
    "\n",
    "    if idx is None:\n",
    "        idx = np.random.randint(len(data))\n",
    "\n",
    "    image_path =  \"/kaggle/input/preprocessed/processed/images/\"+ f\"{data[idx]['image_id']}.png\"\n",
    "    image = Image.open(image_path)\n",
    "    display.display(image)\n",
    "\n",
    "    question = data[idx][\"question\"]\n",
    "    answer = data[idx][\"answer\"]\n",
    "    label = data[idx][\"label\"]\n",
    "\n",
    "    print(f\"Questions : {question}\")\n",
    "    print(f\"Answers : {answer} (Answer_Label: {label})\")\n",
    "\n",
    "    return answer\n",
    "show_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MultimodalCollator:\n",
    "    tokenizer: AutoTokenizer\n",
    "    preprocessor: AutoFeatureExtractor\n",
    "    \n",
    "    def tokenize_text(self, texts: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        encoded_text = self.tokenizer(\n",
    "            text=texts,\n",
    "            padding='longest',\n",
    "            max_length=24,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded_text['input_ids'].squeeze(),\n",
    "            \"token_type_ids\": encoded_text['token_type_ids'].squeeze(),\n",
    "            \"attention_mask\": encoded_text['attention_mask'].squeeze(),\n",
    "        }\n",
    "    \n",
    "    def preprocess_images(self, images: List[str]) -> Dict[str, torch.Tensor]:\n",
    "        processed_images = self.preprocessor(\n",
    "            images=[\n",
    "                Image.open(os.path.join(\"/kaggle/input/preprocessed/processed/images/\", f\"{image_id}.png\")).convert('RGB')\n",
    "                for image_id in images\n",
    "            ],\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": processed_images['pixel_values'].squeeze(),\n",
    "        }\n",
    "            \n",
    "    def __call__(self, raw_batch_dict) -> Dict[str, torch.Tensor]:\n",
    "        question_batch = raw_batch_dict['question'] if isinstance(raw_batch_dict, dict) else [i['question'] for i in raw_batch_dict]\n",
    "        image_id_batch = raw_batch_dict['image_id'] if isinstance(raw_batch_dict, dict) else [i['image_id'] for i in raw_batch_dict]\n",
    "        label_batch = raw_batch_dict['label'] if isinstance(raw_batch_dict, dict) else [i['label'] for i in raw_batch_dict]\n",
    "\n",
    "        return {\n",
    "            **self.tokenize_text(question_batch),\n",
    "            **self.preprocess_images(image_id_batch),\n",
    "            'labels': torch.tensor(label_batch, dtype=torch.int64),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wup_measure(a, b, similarity_threshold=0.925):\n",
    "    def get_semantic_field(word):\n",
    "        weight = 1.0\n",
    "        semantic_field = wordnet.synsets(word, pos=wordnet.NOUN)\n",
    "        return semantic_field, weight\n",
    "\n",
    "    def get_stem_word(word):\n",
    "        weight = 1.0\n",
    "        return word, weight\n",
    "\n",
    "    global_weight = 1.0\n",
    "    a, global_weight_a = get_stem_word(a)\n",
    "    b, global_weight_b = get_stem_word(b)\n",
    "    global_weight = min(global_weight_a, global_weight_b)\n",
    "    if a == b:\n",
    "        return 1.0 * global_weight\n",
    "    if a == \"\" or b == \"\":\n",
    "        return 0\n",
    "    interp_a, weight_a = get_semantic_field(a)\n",
    "    interp_b, weight_b = get_semantic_field(b)\n",
    "    if interp_a == [] or interp_b == []:\n",
    "        return 0\n",
    "    global_max = 0.0\n",
    "    for x in interp_a:\n",
    "        for y in interp_b:\n",
    "            local_score = x.wup_similarity(y)\n",
    "            if local_score > global_max:\n",
    "                global_max = local_score\n",
    "    if global_max < similarity_threshold:\n",
    "        interp_weight = 0.1\n",
    "    else:\n",
    "        interp_weight = 1.0\n",
    "\n",
    "    final_score = global_max * weight_a * weight_b * interp_weight * global_weight\n",
    "    return final_score\n",
    "\n",
    "def batch_wup_measure(labels, preds):\n",
    "    wup_scores = [wup_measure(answer_space[label], answer_space[pred]) for label, pred in zip(labels, preds)]\n",
    "    return np.mean(wup_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you don't want to use Lora execute form here till you reach the heading of with Lora. If you want to use Lora skip till you reach the heading With Lora and execute all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_labels: int = len(answer_space),\n",
    "        intermediate_dim: int = 512,\n",
    "        pretrained_text_name: str = 'bert-base-uncased',\n",
    "        pretrained_image_name: str = 'facebook/deit-base-distilled-patch16-224'\n",
    "    ):\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.pretrained_text_name = pretrained_text_name\n",
    "        self.pretrained_image_name = pretrained_image_name\n",
    "        \n",
    "        self.text_encoder = AutoModel.from_pretrained(self.pretrained_text_name)\n",
    "        self.image_encoder = AutoModel.from_pretrained(self.pretrained_image_name)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.FloatTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None\n",
    "    ):\n",
    "        encoded_text = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        encoded_image = self.image_encoder(\n",
    "            pixel_values=pixel_values,\n",
    "            return_dict=True,\n",
    "        )\n",
    "        fused_output = self.fusion(\n",
    "            torch.cat(\n",
    "                [\n",
    "                    encoded_text['pooler_output'],\n",
    "                    encoded_image['pooler_output'],\n",
    "                ],\n",
    "                dim=1\n",
    "            )\n",
    "        )\n",
    "        logits = self.classifier(fused_output)\n",
    "        \n",
    "        out = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            out[\"loss\"] = loss\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_vqa_collator_and_model(text_encoder='bert-base-uncased', image_encoder='facebook/deit-base-distilled-patch16-224'):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
    "    \n",
    "    \n",
    "    multimodal_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "    multimodal_model = MultimodalVQAModel(\n",
    "        pretrained_text_name=text_encoder,\n",
    "        pretrained_image_name=image_encoder\n",
    "    ).to(device)\n",
    "\n",
    "    return multimodal_collator, multimodal_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.random.randint(len(answer_space), size=5)\n",
    "preds = np.random.randint(len(answer_space), size=5)\n",
    "\n",
    "def showAnswers(ids):\n",
    "    print([answer_space[id] for id in ids])\n",
    "\n",
    "showAnswers(labels)\n",
    "showAnswers(preds)\n",
    "\n",
    "print(\"Predictions vs Labels: \", batch_wup_measure(labels, preds))\n",
    "print(\"Labels vs Labels: \", batch_wup_measure(labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    metrics = {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro'),\n",
    "        \"precision\": precision_score(labels, preds, average='macro'),\n",
    "        \"recall\": recall_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/checkpoint/\",            # Output directory for checkpoints and logs=\n",
    "    seed=12345,                         # Seed for reproducibility\n",
    "    evaluation_strategy=\"epoch\",        # Evaluation strategy: \"steps\" or \"epoch\"\n",
    "    eval_steps=100,                     # Evaluate every 100 steps\n",
    "    logging_strategy=\"epoch\",           # Logging strategy: \"steps\" or \"epoch\"\n",
    "    logging_steps=100,                  # Log every 100 steps\n",
    "    save_strategy=\"epoch\",              # Saving strategy: \"steps\" or \"epoch\"\n",
    "    save_steps=100,                     # Save every 100 steps\n",
    "    save_total_limit=3,                 # Save only the last 3 checkpoints at any given time during training \n",
    "    metric_for_best_model='wups',       # Metric used for determining the best model\n",
    "    per_device_train_batch_size=32,     # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=32,      # Batch size per GPU for evaluation\n",
    "    remove_unused_columns=False,        # Whether to remove unused columns in the dataset\n",
    "    num_train_epochs=20,                 # Number of training epochs\n",
    "    fp16=True,                          # Enable mixed precision training (float16)\n",
    "    dataloader_num_workers=8,           # Number of workers for data loading\n",
    "    load_best_model_at_end=True,        # Whether to load the best model at the end of training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(dataset, args, text_model='bert-base-uncased', image_model='microsoft/beit-base-patch16-224-pt22k-ft22k', multimodal_model='bert_deit'):\n",
    "    \n",
    "    print(text_model,image_model)\n",
    "    collator, model = create_multimodal_vqa_collator_and_model(text_model, image_model)\n",
    "    multi_args = deepcopy(args)\n",
    "    multi_args.output_dir = os.path.join(\"/kaggle/working/checkpoint/\", multimodal_model)\n",
    "    print(multi_args.output_dir)\n",
    "    multi_trainer = Trainer(\n",
    "        model,\n",
    "        multi_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "    \n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics, multi_trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Models : \n",
    "\n",
    "Bert : bert-base-uncased\n",
    "\n",
    "Roberta : roberta-base\n",
    "\n",
    "## Image Models :\n",
    "\n",
    "ViT : google/vit-base-patch16-224\n",
    "\n",
    "DeIT : facebook/deit-base-distilled-patch16-224\n",
    "\n",
    "BeIT : microsoft/beit-base-patch16-224-pt22k-ft22k\n",
    "\n",
    "You can use any combo here by replacing text_model and image_model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics, trainer = create_and_train_model( dataset, args,text_model='roberta-base',image_model='microsoft/beit-base-patch16-224-pt22k-ft22k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multi_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Roberta_BeIT_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameters: {:,}\".format(num_params))\n",
    "count_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRaRobertaModel(nn.Module):\n",
    "    def __init__(self, model_name='roberta-base', rank=32, lora_alpha=32, lora_dropout=0.1):\n",
    "        super(LoRaRobertaModel, self).__init__()\n",
    "        self.config = RobertaConfig.from_pretrained(model_name)\n",
    "        self.config.lora = True\n",
    "        self.config.lora_rank = rank\n",
    "        self.config.lora_alpha = lora_alpha\n",
    "        self.config.lora_dropout = lora_dropout\n",
    "        self.roberta = RobertaModel(self.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        return self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "model = LoRaRobertaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRaBertModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', rank=16, lora_alpha=32, lora_dropout=0.1):\n",
    "        super(LoRaBertModel, self).__init__()\n",
    "        self.config = BertConfig.from_pretrained(model_name)\n",
    "        self.config.lora = True\n",
    "        self.config.lora_rank = rank\n",
    "        self.config.lora_alpha = lora_alpha\n",
    "        self.config.lora_dropout = lora_dropout\n",
    "        self.bert = BertModel(self.config)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        return self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoFeatureExtractor, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRaViTModel(nn.Module):\n",
    "    def __init__(self, model_name='google/vit-base-patch16-224', rank=32, lora_alpha=32, lora_dropout=0.1):\n",
    "        super(LoRaViTModel, self).__init__()\n",
    "        self.config = ViTConfig.from_pretrained(model_name)\n",
    "        self.config.lora = True\n",
    "        self.config.lora_rank = rank\n",
    "        self.config.lora_alpha = lora_alpha\n",
    "        self.config.lora_dropout = lora_dropout\n",
    "        self.vit = ViTModel(self.config)\n",
    "\n",
    "    def forward(self, pixel_values, attention_mask=None):\n",
    "        outputs = self.vit(pixel_values)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DeiTConfig, DeiTModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRaDeiTModel(nn.Module):\n",
    "    def __init__(self, model_name='facebook/deit-base-patch16-224', rank=32, lora_alpha=32, lora_dropout=0.1):\n",
    "        super(LoRaDeiTModel, self).__init__()\n",
    "        self.config = DeiTConfig.from_pretrained(model_name)\n",
    "        self.config.lora = True\n",
    "        self.config.lora_rank = rank\n",
    "        self.config.lora_alpha = lora_alpha\n",
    "        self.config.lora_dropout = lora_dropout\n",
    "        self.deit = DeiTModel(self.config)\n",
    "\n",
    "    def forward(self, pixel_values, attention_mask=None):\n",
    "        outputs = self.deit(pixel_values)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BeitConfig, BeitModel\n",
    "class LoRaBeitModel(nn.Module):\n",
    "    def __init__(self, model_name='microsoft/beit-base-patch16-224', rank=32, lora_alpha=32, lora_dropout=0.1):\n",
    "        super(LoRaBeitModel, self).__init__()\n",
    "        self.config = BeitConfig.from_pretrained(model_name)\n",
    "        self.config.lora = True\n",
    "        self.config.lora_rank = rank\n",
    "        self.config.lora_alpha = lora_alpha\n",
    "        self.config.lora_dropout = lora_dropout\n",
    "        self.beit = BeitModel(self.config)\n",
    "\n",
    "    def forward(self, pixel_values, attention_mask=None):\n",
    "        outputs = self.beit(pixel_values)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here , if you want to use any combo , then use the corresponding text model and image model in MultimodalVQAModel\n",
    "\n",
    "## like  in  multimodelVQAModel class,\n",
    "\n",
    "just change the self.text_encoder and self.image_encoder  to the required Class. Like we have used Bert and Deit here. One change change it using the above defined classes. Params will remain the same. Chnage the rank manually here. Defualt is 32 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalVQAModel(nn.Module):\n",
    "    def __init__(self, num_labels, intermediate_dim=512,pretrained_text_name = 'bert-base-uncased', pretrained_image_name='google/vit-base-patch16-224-in21k',rank = 32):\n",
    "        super(MultimodalVQAModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.text_encoder = LoRaBertModel(pretrained_text_name, rank=rank, lora_alpha=16, lora_dropout=0.1)\n",
    "        self.image_encoder = LoRaDeiTModel(pretrained_image_name, rank=rank, lora_alpha=16, lora_dropout=0.1)\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(self.text_encoder.config.hidden_size + self.image_encoder.config.hidden_size, intermediate_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "        self.classifier = nn.Linear(intermediate_dim, self.num_labels)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, pixel_values, attention_mask=None, token_type_ids=None, labels=None):\n",
    "        text_output = self.text_encoder(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        image_output = self.image_encoder(pixel_values)\n",
    "        fused_output = self.fusion(torch.cat([text_output.pooler_output, image_output.pooler_output], dim=1))\n",
    "        logits = self.classifier(fused_output)\n",
    "        output = {\"logits\": logits}\n",
    "        if labels is not None:\n",
    "            loss = self.criterion(logits, labels)\n",
    "            output[\"loss\"] = loss\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multimodal_vqa_collator_and_model(text_encoder='bert-base-uncased',image_encoder='google/vit-base-patch16-224-in21k'):\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_encoder)\n",
    "    preprocessor = AutoFeatureExtractor.from_pretrained(image_encoder)\n",
    "    \n",
    "    multimodal_collator = MultimodalCollator(\n",
    "        tokenizer=tokenizer,\n",
    "        preprocessor=preprocessor,\n",
    "    )\n",
    "\n",
    "    multimodal_model = MultimodalVQAModel(\n",
    "        num_labels=len(answer_space), \n",
    "        intermediate_dim=512,\n",
    "         pretrained_text_name=text_encoder,\n",
    "        pretrained_image_name=image_encoder\n",
    "    ).to(device)\n",
    "\n",
    "    return multimodal_collator, multimodal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_tuple: Tuple[np.ndarray, np.ndarray]) -> Dict[str, float]:\n",
    "    \n",
    "    logits, labels = eval_tuple\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    metrics = {\n",
    "        \"wups\": batch_wup_measure(labels, preds),\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average='macro'),\n",
    "        \"precision\": precision_score(labels, preds, average='macro'),\n",
    "        \"recall\": recall_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"/kaggle/working/checkpoint/\",            # Output directory for checkpoints and logs=\n",
    "    seed=12345,                         # Seed for reproducibility\n",
    "    evaluation_strategy=\"epoch\",        # Evaluation strategy: \"steps\" or \"epoch\"\n",
    "    eval_steps=100,                     # Evaluate every 100 steps\n",
    "    logging_strategy=\"epoch\",           # Logging strategy: \"steps\" or \"epoch\"\n",
    "    logging_steps=100,                  # Log every 100 steps\n",
    "    save_strategy=\"epoch\",              # Saving strategy: \"steps\" or \"epoch\"\n",
    "    save_steps=100,                     # Save every 100 steps\n",
    "    save_total_limit=3,                 # Save only the last 3 checkpoints at any given time during training \n",
    "    metric_for_best_model='wups',       # Metric used for determining the best model\n",
    "    per_device_train_batch_size=32,     # Batch size per GPU for training\n",
    "    per_device_eval_batch_size=32,      # Batch size per GPU for evaluation\n",
    "    remove_unused_columns=False,        # Whether to remove unused columns in the dataset\n",
    "    num_train_epochs=20,                 # Number of training epochs\n",
    "    fp16=True,                          # Enable mixed precision training (float16)\n",
    "    dataloader_num_workers=8,           # Number of workers for data loading\n",
    "    load_best_model_at_end=True,        # Whether to load the best model at the end of training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_train_model(dataset, args, text_model='roberta-base',image_model=\"microsoft/beit-base-patch16-224-pt22k-ft22k\", multimodal_model='bert_vit'):\n",
    "    \n",
    "    print(text_model,image_model)\n",
    "    collator, model = create_multimodal_vqa_collator_and_model(text_model,image_model)\n",
    "    \n",
    "    \n",
    "    multi_args = deepcopy(args)\n",
    "    multi_args.output_dir = os.path.join(\"/kaggle/working/checkpoint/\", multimodal_model)\n",
    "    print(multi_args.output_dir)\n",
    "\n",
    "    \n",
    "    multi_trainer = Trainer(\n",
    "        model,\n",
    "        multi_args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['test'],\n",
    "        data_collator=collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    \n",
    "    train_multi_metrics = multi_trainer.train()\n",
    "    eval_multi_metrics = multi_trainer.evaluate()\n",
    "    \n",
    "    return collator, model, train_multi_metrics, eval_multi_metrics, multi_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator, model, train_multi_metrics, eval_multi_metrics, trainer = create_and_train_model( dataset, args,text_model='bert-base-uncased',image_model=\"microsoft/beit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_multi_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'Bert_BeIT_weights_lora_32_new.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(\"Number of trainable parameters: {:,}\".format(num_params))\n",
    "count_trainable_parameters(model)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1876338,
     "sourceId": 3064985,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
